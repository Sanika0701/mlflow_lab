{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison Dashboard\n",
    "# This notebook creates comprehensive model comparison visualizations\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"üìä Model Comparison Dashboard\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Connect to MLflow\n",
    "client = MlflowClient()\n",
    "experiment = mlflow.get_experiment_by_name(\"wine_quality_comparison\")\n",
    "\n",
    "if experiment is None:\n",
    "    print(\"‚ùå Experiment 'wine_quality_comparison' not found!\")\n",
    "    print(\"   Please run 02_model_training.ipynb first.\")\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "    print(f\"‚úÖ Found experiment: {experiment.name} (ID: {experiment_id})\")\n",
    "    \n",
    "    # Get all runs\n",
    "    runs = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "    print(f\"   Total runs: {len(runs)}\")\n",
    "    \n",
    "    # Extract key metrics\n",
    "    comparison_data = []\n",
    "    for _, run in runs.iterrows():\n",
    "        comparison_data.append({\n",
    "            'Model': run['params.model_type'],\n",
    "            'Run Name': run['tags.mlflow.runName'],\n",
    "            'AUC': run['metrics.auc'],\n",
    "            'Accuracy': run['metrics.accuracy'],\n",
    "            'Precision': run['metrics.precision'],\n",
    "            'Recall': run['metrics.recall'],\n",
    "            'F1 Score': run['metrics.f1_score'],\n",
    "            'Run ID': run['run_id']\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Display comparison table\n",
    "    print(\"\\nüìã Model Performance Comparison:\")\n",
    "    print(\"-\" * 60)\n",
    "    display(df_comparison.sort_values('AUC', ascending=False))\n",
    "    \n",
    "    # 1. AUC Comparison\n",
    "    print(\"\\nüìä Visualization 1: AUC Score Comparison\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.barplot(data=df_comparison.sort_values('AUC', ascending=False), \n",
    "                     x='Model', y='AUC', palette='viridis')\n",
    "    plt.title('Model Comparison - AUC Scores', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('AUC Score', fontsize=12)\n",
    "    plt.ylim(0.7, 1.0)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.4f', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Multi-metric comparison\n",
    "    print(\"\\nüìä Visualization 2: Multi-Metric Comparison\")\n",
    "    metrics_to_plot = ['AUC', 'Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(20, 4))\n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        ax = axes[idx]\n",
    "        df_sorted = df_comparison.sort_values(metric, ascending=False)\n",
    "        ax.barh(df_sorted['Model'], df_sorted[metric], color=plt.cm.viridis(idx/len(metrics_to_plot)))\n",
    "        ax.set_xlabel(metric, fontsize=10, fontweight='bold')\n",
    "        ax.set_xlim(0.7, 1.0)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(df_sorted[metric]):\n",
    "            ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Comprehensive Model Performance Comparison', \n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Radar chart for all metrics\n",
    "    print(\"\\nüìä Visualization 3: Radar Chart - Overall Performance\")\n",
    "    from math import pi\n",
    "    \n",
    "    categories = ['AUC', 'Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    N = len(categories)\n",
    "    \n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    colors = plt.cm.Set3(range(len(df_comparison)))\n",
    "    \n",
    "    for idx, row in df_comparison.iterrows():\n",
    "        values = [row['AUC'], row['Accuracy'], row['Precision'], \n",
    "                 row['Recall'], row['F1 Score']]\n",
    "        values += values[:1]\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[idx])\n",
    "        ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, size=12)\n",
    "    ax.set_ylim(0.7, 1.0)\n",
    "    ax.set_title('Model Performance Radar Chart', size=16, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Best model summary\n",
    "    best_model = df_comparison.loc[df_comparison['AUC'].idxmax()]\n",
    "    print(\"\\nüèÜ Best Model Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"   Model: {best_model['Model']}\")\n",
    "    print(f\"   Run Name: {best_model['Run Name']}\")\n",
    "    print(f\"   AUC: {best_model['AUC']:.4f}\")\n",
    "    print(f\"   Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "    print(f\"   Precision: {best_model['Precision']:.4f}\")\n",
    "    print(f\"   Recall: {best_model['Recall']:.4f}\")\n",
    "    print(f\"   F1 Score: {best_model['F1 Score']:.4f}\")\n",
    "    print(f\"   Run ID: {best_model['Run ID']}\")\n",
    "    \n",
    "    # 5. Save comparison report\n",
    "    print(\"\\nüíæ Saving comparison report...\")\n",
    "    df_comparison.to_csv('model_comparison_report.csv', index=False)\n",
    "    print(\"   ‚úÖ Saved to: model_comparison_report.csv\")\n",
    "    \n",
    "    print(\"\\nüéØ Next step: Run 04_model_monitoring.ipynb to monitor the best model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
